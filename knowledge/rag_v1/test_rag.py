import os
import argparse
import shutil
from dotenv import load_dotenv
from src.parser import DocumentParser
from src.api import get_rag_api
from src.config import config

def run_parse_test(doc_path: str):
    """æµ‹è¯•å•ä¸ªæ–‡æ¡£çš„åˆ†å—åŠŸèƒ½"""
    print(f"==============\nâ–¶ï¸  Running Parse Test: {doc_path}\n==============")
    if not os.path.exists(doc_path):
        print(f"âŒ FAILED: Document not found at '{doc_path}'")
        return

    parser = DocumentParser(
        chunk_size=config.MAX_CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        use_h1_only=config.USE_H1_ONLY
    )
    
    docs = parser.load_documents(os.path.dirname(doc_path))
    # æ‰¾åˆ°æˆ‘ä»¬å…³å¿ƒçš„é‚£ä¸€ä¸ªæ–‡æ¡£
    target_doc = next((d for d in docs if d.metadata.get("file_path") == doc_path), None)

    if not target_doc:
        print(f"âŒ FAILED: Could not load the specific document from its directory.")
        return

    nodes = parser.parse_documents([target_doc])

    print(f"âœ… Document parsed into {len(nodes)} chunks.\n")
    for i, node in enumerate(nodes):
        print(f"--- Chunk {i+1} (Length: {len(node.get_text())}) ---")
        print(node.get_text())
        print("-" * 20 + "\n")

def run_retrieve_test(keyword: str):
    """æµ‹è¯•çº¯æ£€ç´¢åŠŸèƒ½ï¼ˆåªä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼Œä¸åˆå§‹åŒ– LLMï¼‰"""
    print(f"==============\nâ–¶ï¸  Running Retrieve Test with keyword '{keyword}'\n==============")
    
    # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
    from src.db import get_collection_stats, get_storage_context, get_vector_store_index
    stats = get_collection_stats(config.KNOWLEDGE_BASE_PATH, config.CHROMA_COLLECTION_NAME)
    
    if stats.get("total_documents", 0) == 0:
        print("âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–ä¸ºç©ºï¼")
        print("\nè¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ æ–‡æ¡£ï¼š")
        print("  python3 test_rag.py embed --doc /path/to/document.md")
        print("\næˆ–æ‰¹é‡åˆå§‹åŒ–ï¼š")
        print("  python3 rag_cli.py init")
        return
    
    print(f"âœ… çŸ¥è¯†åº“å·²åŠ è½½ï¼Œå…± {stats['total_documents']} ä¸ªæ–‡æ¡£\n")
    
    try:
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        # åªé…ç½®åµŒå…¥æ¨¡å‹ï¼ˆä¸éœ€è¦ LLMï¼‰
        embed_model = OpenAIEmbedding(
            api_key=config.OPENAI_API_KEY,
            api_base=config.OPENAI_BASE_URL,
            model_name=config.EMBEDDING_MODEL,
            embed_batch_size=32
        )
        
        # åŠ è½½ç´¢å¼•
        storage_context = get_storage_context(
            persist_dir=config.KNOWLEDGE_BASE_PATH,
            collection_name=config.CHROMA_COLLECTION_NAME
        )
        index = get_vector_store_index(storage_context, embed_model=embed_model)
        
        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆä¸éœ€è¦ LLMï¼‰
        retriever = index.as_retriever(similarity_top_k=config.TOP_K)
        
        # æ‰§è¡Œæ£€ç´¢
        nodes = retriever.retrieve(keyword)
        
        print(f"âœ… Found {len(nodes)} sources.")
        for i, node in enumerate(nodes, 1):
            print(f"\n--- Source {i} (Similarity: {node.score:.3f}) ---")
            print(f"Title: {node.metadata.get('title', node.metadata.get('file_name', 'æœªçŸ¥'))}")
            print(f"H1 Title: {node.metadata.get('h1_title', 'N/A')}")
            text = node.get_text()
            print(f"Snippet: {text[:200]}...")
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

def run_query_test(question: str):
    """æµ‹è¯•å®Œæ•´æŸ¥è¯¢åŠŸèƒ½ï¼Œä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“"""
    print(f"==============\nâ–¶ï¸  Running Query Test with question '{question}'\n==============")

    # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
    from src.db import get_collection_stats
    stats = get_collection_stats(config.KNOWLEDGE_BASE_PATH, config.CHROMA_COLLECTION_NAME)
    
    if stats.get("total_documents", 0) == 0:
        print("âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–ä¸ºç©ºï¼")
        print("\nè¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤åˆå§‹åŒ–çŸ¥è¯†åº“ï¼š")
        print("  python3 rag_cli.py init")
        print("\næˆ–ä½¿ç”¨ embed å‘½ä»¤æµ‹è¯•å•ä¸ªæ–‡æ¡£ï¼š")
        print("  python3 test_rag.py embed --doc /path/to/document.md")
        return
    
    print(f"âœ… çŸ¥è¯†åº“å·²åŠ è½½ï¼Œå…± {stats['total_documents']} ä¸ªæ–‡æ¡£\n")

    try:
        api = get_rag_api()
        
        result = api.query(question=question, include_answer=True)
        
        if not result.get("success"):
            print("âŒ FAILED: Full query failed.")
            print(f"Error: {result.get('error', 'Unknown error')}")
            return
            
        data = result.get("data", {})
        print(f"\nğŸ’¡ Answer:\n{data.get('answer')}")
        print("\nğŸ“š Sources:")
        for i, source in enumerate(data.get('sources', []), 1):
            print(f"  - Source {i}: {source.get('title')} (Similarity: {source.get('similarity', 0.0):.3f})")
    except Exception as e:
        print(f"âŒ Error: {e}")

def run_embed_test(doc_path: str):
    """æµ‹è¯•å•ä¸ªæ–‡æ¡£çš„åµŒå…¥å’Œå…ƒæ•°æ®éªŒè¯ï¼Œæ•°æ®ä¿å­˜åˆ°æ­£å¼çŸ¥è¯†åº“"""
    print(f"==============\nâ–¶ï¸  Running Embed Test: {doc_path}\n==============")
    
    if not os.path.exists(doc_path):
        print(f"âŒ FAILED: Document not found at '{doc_path}'")
        return
    
    print(f"ğŸ“¦ Using production database: {config.KNOWLEDGE_BASE_PATH}\n")
    
    try:
        # 1. è§£ææ–‡æ¡£
        parser = DocumentParser(
            chunk_size=config.MAX_CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            use_h1_only=config.USE_H1_ONLY
        )
        
        docs = parser.load_documents(os.path.dirname(doc_path))
        target_doc = next((d for d in docs if doc_path in d.metadata.get("file_path", "")), None)
        
        if not target_doc:
            print(f"âŒ FAILED: Could not load the specific document.")
            return
        
        print(f"âœ… Document loaded successfully.\n")
        print(f"ğŸ“‹ Original Document Metadata (from YAML frontmatter):")
        for key, value in target_doc.metadata.items():
            print(f"  - {key}: {value}")
        
        # 2. è§£æä¸ºèŠ‚ç‚¹
        nodes = parser.parse_documents([target_doc])
        print(f"\nâœ… Document parsed into {len(nodes)} chunks.\n")
        
        # 3. æ˜¾ç¤ºç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
        if nodes:
            first_node = nodes[0]
            print(f"{'=' * 80}")
            print(f"ğŸ“„ First Chunk Details")
            print(f"{'=' * 80}")
            print(f"\nğŸ“‹ Node Metadata:")
            for key, value in first_node.metadata.items():
                print(f"  - {key}: {value}")
            
            print(f"\nğŸ“ Node Text (first 300 chars):")
            print(f"{'-' * 80}")
            text = first_node.get_text()
            print(text[:300] + "..." if len(text) > 300 else text)
            print(f"{'-' * 80}")
        
        # 4. åˆ›å»ºå‘é‡ç´¢å¼•å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        print(f"\n{'=' * 80}")
        print(f"ğŸ”„ Creating vector index and saving to database...")
        print(f"{'=' * 80}\n")
        
        from llama_index.core import Settings as LlamaSettings
        from llama_index.embeddings.openai import OpenAIEmbedding
        from src.db import get_storage_context, get_vector_store_index
        
        # é…ç½®åµŒå…¥æ¨¡å‹ï¼ˆåªéœ€è¦åµŒå…¥ï¼Œä¸éœ€è¦ LLMï¼‰
        embed_model = OpenAIEmbedding(
            api_key=config.OPENAI_API_KEY,
            api_base=config.OPENAI_BASE_URL,
            model_name=config.EMBEDDING_MODEL,
            embed_batch_size=32
        )
        
        # è·å–å­˜å‚¨ä¸Šä¸‹æ–‡
        storage_context = get_storage_context(
            persist_dir=config.KNOWLEDGE_BASE_PATH,
            collection_name=config.CHROMA_COLLECTION_NAME
        )
        
        # åˆ›å»ºæˆ–è·å–ç´¢å¼•
        index = get_vector_store_index(storage_context, embed_model=embed_model)
        
        # æ’å…¥èŠ‚ç‚¹
        index.insert_nodes(nodes)
        
        print(f"âœ… Successfully embedded and saved {len(nodes)} chunks to database.")
        
        # 5. ç›´æ¥æŸ¥è¯¢æ•°æ®åº“éªŒè¯æ•°æ®å’Œå…ƒæ•°æ®
        print(f"\n{'=' * 80}")
        print(f"ğŸ” Querying database directly to verify data...")
        print(f"{'=' * 80}\n")
        
        from src.db import get_collection_data, get_collection_stats
        
        # å…ˆæ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
        stats = get_collection_stats(config.KNOWLEDGE_BASE_PATH, config.CHROMA_COLLECTION_NAME)
        print(f"ğŸ“Š Database Statistics:")
        print(f"  - Total documents in DB: {stats.get('total_documents', 0)}")
        
        if stats.get('total_documents', 0) == 0:
            print("\nâš ï¸  Warning: No documents found in database. Data may not have been persisted correctly.")
            print(f"ğŸ’¾ Data should be persisted to: {config.KNOWLEDGE_BASE_PATH}")
            return
        
        # æŸ¥è¯¢æ•°æ®åº“å†…å®¹
        db_data = get_collection_data(config.KNOWLEDGE_BASE_PATH, config.CHROMA_COLLECTION_NAME, limit=min(len(nodes), 10))
        
        if 'error' in db_data:
            print(f"âŒ Error querying database: {db_data['error']}")
        else:
            print(f"  - Documents retrieved: {db_data['count']}")
            print(f"  - Has embeddings: {db_data['has_embeddings']}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            if db_data['count'] > 0:
                print(f"\n{'=' * 80}")
                print(f"ğŸ“„ First Document in Database")
                print(f"{'=' * 80}")
                print(f"\nDocument ID: {db_data['ids'][0]}")
                
                print(f"\nğŸ“‹ Stored Metadata:")
                for key, value in db_data['metadatas'][0].items():
                    # æˆªæ–­è¿‡é•¿çš„å€¼
                    display_value = str(value)
                    if len(display_value) > 100:
                        display_value = display_value[:100] + "..."
                    print(f"  - {key}: {display_value}")
                
                print(f"\nğŸ“ Stored Text (first 300 chars):")
                print(f"{'-' * 80}")
                doc_text = db_data['documents'][0]
                print(doc_text[:300] + "..." if len(doc_text) > 300 else doc_text)
                print(f"{'-' * 80}")
                
                # éªŒè¯å…³é”®å…ƒæ•°æ®å­—æ®µ
                print(f"\nâœ… Metadata Verification:")
                metadata = db_data['metadatas'][0]
                checks = [
                    ("YAML 'id' field", 'id' in metadata),
                    ("YAML 'title' field", 'title' in metadata),
                    ("YAML 'url' field", 'url' in metadata),
                    ("One-level heading 'h1_title'", 'h1_title' in metadata),
                    ("Chunk index", 'chunk_index' in metadata),
                    ("File name", 'file_name' in metadata),
                ]
                
                for check_name, check_result in checks:
                    status = "âœ“" if check_result else "âœ—"
                    print(f"  {status} {check_name}")
        
        print(f"\nâœ¨ Embed test completed successfully!\n")
        
        print(f"\nğŸ’¾ Data persisted to: {config.KNOWLEDGE_BASE_PATH}")
        
    except Exception as e:
        print(f"âŒ Error during embed test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))
    
    default_doc = os.path.join(config.GUIDE_DOCS_PATH, 'mh0pppib5eyc_å°åœ°å›¾æ ‡è¯†.md')

    parser = argparse.ArgumentParser(description="RAG Pipeline Debugging Tool")
    subparsers = parser.add_subparsers(dest='command', required=True)

    # Parser command
    parse_parser = subparsers.add_parser('parse', help='Test document chunking.')
    parse_parser.add_argument('--doc', type=str, default=default_doc, help='Path to the document to parse.')

    # Retrieve command
    retrieve_parser = subparsers.add_parser('retrieve', help='Test keyword retrieval using existing knowledge base.')
    retrieve_parser.add_argument('keyword', type=str, help='Keyword to retrieve.')
    
    # Query command
    query_parser = subparsers.add_parser('query', help='Test full RAG query using existing knowledge base.')
    query_parser.add_argument('question', type=str, help='Question to ask.')
    
    # Embed command
    embed_parser = subparsers.add_parser('embed', help='Test embedding and metadata verification (saves to production DB).')
    embed_parser.add_argument('--doc', type=str, default=default_doc, help='Path to the document to embed.')

    args = parser.parse_args()

    if args.command == 'parse':
        run_parse_test(args.doc)
    elif args.command == 'retrieve':
        run_retrieve_test(args.keyword)
    elif args.command == 'query':
        run_query_test(args.question)
    elif args.command == 'embed':
        run_embed_test(args.doc)